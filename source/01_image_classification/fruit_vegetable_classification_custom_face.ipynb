{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"fruit&vegetable_classification_custom_face.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPa29QwZrvaCvIukupt+U7F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"myvOZQod9aTe"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import PIL\n","import tensorflow as tf\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential"]},{"cell_type":"code","source":["import pathlib\n","data_dir = pathlib.Path('C:/Users/HERO/Desktop/Graduate school/1학기\\\n","/텐서플로우 활용기초/팀프로젝트/face emotion/Image emotion detection_dataset_1')\n"],"metadata":{"id":"ltQxpsOu9kHx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","img_height = 180\n","img_width = 180"],"metadata":{"id":"2c-n4Rgd9kJ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.2,\n","  subset=\"training\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)\n"],"metadata":{"id":"KSxKtW8_9kMB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.2,\n","  subset=\"validation\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)"],"metadata":{"id":"Ehq8DBSg9kOM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class_names = train_ds.class_names\n","print(class_names)"],"metadata":{"id":"RbpNsCmj9kQI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.figure(figsize=(10, 10))\n","for images, labels in train_ds.take(1):\n","    for i in range(9):\n","        ax = plt.subplot(3, 3, i + 1)\n","        plt.imshow(images[i].numpy().astype(\"uint8\"))\n","        plt.title(class_names[labels[i]])\n","        plt.axis(\"off\")"],"metadata":{"id":"PKJ3QsdN9kSK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for image_batch, labels_batch in train_ds:\n","    print(image_batch.shape)\n","    print(labels_batch.shape)\n","    break"],"metadata":{"id":"Lpmhi6oo9kUQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["AUTOTUNE = tf.data.experimental.AUTOTUNE\n","\n","train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n","val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"],"metadata":{"id":"D0jV43Ov9kWc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_classes = 7\n","\n","model = Sequential([\n","  layers.experimental.preprocessing.Rescaling(1./255),\n","  layers.Conv2D(16, 3, padding='same', activation='relu'),\n","  layers.MaxPooling2D(),\n","  layers.Conv2D(32, 3, padding='same', activation='relu'),\n","  layers.MaxPooling2D(),\n","  layers.Conv2D(64, 3, padding='same', activation='relu'),\n","  layers.MaxPooling2D(),\n","  layers.Dropout(0.2),\n","  layers.Flatten(),\n","  layers.Dense(128, activation='relu'),\n","  layers.Dense(num_classes)\n","])"],"metadata":{"id":"ziT0U-_p9kYf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])"],"metadata":{"id":"bkVqxzpE9kay"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"l7s-W3IU9kc1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 20\n","history = model.fit(\n","  train_ds,\n","  validation_data=val_ds,\n","  epochs=epochs\n",")"],"metadata":{"id":"W5vdWRD99kfD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs_range = range(epochs)\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"],"metadata":{"id":"tfoYjyJb9ktZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_path ='C:/Users/HERO/Desktop/Graduate school/1학기\\\n","/텐서플로우 활용기초/팀프로젝트/face emotion/prediction/Predict_image1.jpg'\n","\n","img = keras.preprocessing.image.load_img(\n","    image_path, target_size=(img_height, img_width)\n",")\n","img_array = keras.preprocessing.image.img_to_array(img)\n","img_array = tf.expand_dims(img_array, 0) # Create a batch\n","\n","predictions = model.predict(img_array)\n","score = tf.nn.softmax(predictions[0])\n","\n","print(\n","    \" This image most likely belongs to {} with a {:.2f} percent confidence.\"\n","    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",")\n","\n"],"metadata":{"id":"lDgq7OSs9kvq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"clE0jYWE9kyu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"lX8Xyb3w9k0o"},"execution_count":null,"outputs":[]}]}