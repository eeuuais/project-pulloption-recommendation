{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"fruit&vegetable_classification_VGGNet_light version_food.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOb7wuc3dW/MVxyF0IkDQvL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"FZpbizQQ-0OH"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import PIL\n","import tensorflow as tf\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","\n","from tensorflow.keras import Model\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.losses import categorical_crossentropy\n","from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n"]},{"cell_type":"code","source":["import pathlib\n","data_dir = pathlib.Path('C:/Users/HERO/Desktop/Graduate school/1학기\\\n","/텐서플로우 활용기초/팀프로젝트/food detection/Fruit_vegetable')\n"],"metadata":{"id":"J5sqAi0r-3r9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","img_height = 224\n","img_width = 224"],"metadata":{"id":"gJHPS0Mp-3uA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.2,\n","  subset=\"training\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)\n"],"metadata":{"id":"eZv6Kxmg-3yL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.2,\n","  subset=\"validation\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)"],"metadata":{"id":"-O7A9jSY-30c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class_names = train_ds.class_names\n","print(class_names)"],"metadata":{"id":"4Azm61rF-32w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.figure(figsize=(10, 10))\n","for images, labels in train_ds.take(1):\n","    for i in range(9):\n","        ax = plt.subplot(3, 3, i + 1)\n","        plt.imshow(images[i].numpy().astype(\"uint8\"))\n","        plt.title(class_names[labels[i]])\n","        plt.axis(\"off\")"],"metadata":{"id":"k3BlKAK--349"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for image_batch, labels_batch in train_ds:\n","    print(image_batch.shape)\n","    print(labels_batch.shape)\n","    break"],"metadata":{"id":"GD4V-Oeo-363"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["AUTOTUNE = tf.data.experimental.AUTOTUNE\n","\n","train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n","val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"],"metadata":{"id":"ZFmHXjld-39A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_classes = 50\n","input_shape=(224,224,3)\n","\n","model_VGGNet = Sequential([\n","  layers.Conv2D(64, kernel_size=(3,3), padding='same',\n","                       activation='relu', input_shape=input_shape),\n","  layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n","    \n","  layers.Conv2D(128, kernel_size=(3,3), padding='same',\n","                       activation='relu'),\n","  layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n","    \n","  layers.Conv2D(256, kernel_size=(3,3), padding='same',\n","                       activation='relu'),\n","  layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n","    \n","  layers.Conv2D(512, kernel_size=(3,3), padding='same',\n","                       activation='relu'),\n","  layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n","    \n","  layers.Conv2D(512, kernel_size=(3,3), padding='same',\n","                       activation='relu'),\n","  layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n","    \n","  layers.Flatten(),\n","  layers.Dense(4096, activation='relu'),\n","  layers.Dropout(0.5),\n","  layers.Dense(4096, activation='relu'),\n","  layers.Dropout(0.5),\n","  layers.Dense(num_classes, activation='softmax')\n","])\n","\n"],"metadata":{"id":"Ipn7LwLN-3-z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_VGGNet.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])"],"metadata":{"id":"UIavZ0Ec-4BM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_VGGNet.summary()"],"metadata":{"id":"WJqKpRae-4Dc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 20\n","history = model_VGGNet.fit(\n","  train_ds,\n","  validation_data=val_ds,\n","  epochs=epochs\n",")"],"metadata":{"id":"754ycoOl-4F1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs_range = range(epochs)\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"],"metadata":{"id":"Kg7SIbgs-4H8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","image_path ='C:/Users/HERO/Desktop/Graduate school/1학기\\\n","/텐서플로우 활용기초/팀프로젝트/food detection/prediction/Predict_image1.jpg'\n","\n","img = keras.preprocessing.image.load_img(\n","    image_path, target_size=(img_height, img_width)\n",")\n","img_array = keras.preprocessing.image.img_to_array(img)\n","img_array = tf.expand_dims(img_array, 0) # Create a batch\n","\n","predictions = model_VGGNet.predict(img_array)\n","score = tf.nn.softmax(predictions[0])\n","\n","print(\n","    \" This image most likely belongs to {} with a {:.2f} percent confidence.\"\n","    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",")\n","\n"],"metadata":{"id":"L_U_V_rX-4Kk"},"execution_count":null,"outputs":[]}]}